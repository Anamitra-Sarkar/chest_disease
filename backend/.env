# Groq API Key for LLM interpretation
# Get your key from: https://console.groq.com/
GROQ_API_KEY=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

# Path to the PyTorch model file
# Model is automatically downloaded from Hugging Face Hub during Docker build
# For local development, run: bash download_model.sh
MODEL_PATH=epoch_001_mAUROC_0.486525.pth

# Inference device: 'cpu' or 'cuda'
# Use 'cpu' for compatibility (HuggingFace Spaces default), 'cuda' for GPU acceleration
INFERENCE_DEVICE=cpu

# Port for the backend API
# HuggingFace Spaces uses 7860 by default
PORT=7860

# Frontend API URL (for Next.js, if using frontend)
NEXT_PUBLIC_API_URL=http://localhost:7860
